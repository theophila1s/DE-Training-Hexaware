{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197c793b-fa61-43e2-a67c-53526e959222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+-------------------+-----------+------+\n| id|firstName|middleName|lastName|gender|          birthDate|        ssn|salary|\n+---+---------+----------+--------+------+-------------------+-----------+------+\n|  1|     John|          |     Doe|     M|1990-05-14 00:00:00|111-22-3333| 60000|\n|  2|     Mary|         A|   Smith|     F|1985-08-20 00:00:00|222-33-4444| 75000|\n+---+---------+----------+--------+------+-------------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1) Define schema and sample DataFrame (reuse this for multiple examples)\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"middleName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"birthDate\", TimestampType(), True),\n",
    "    StructField(\"ssn\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"John\", \"\", \"Doe\", \"M\", datetime(1990,5,14), \"111-22-3333\", 60000),\n",
    "    (2, \"Mary\", \"A\", \"Smith\", \"F\", datetime(1985,8,20), \"222-33-4444\", 75000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bdaea96-de0a-4aa3-a59f-8023e98f3809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Delta Table — Method 1: `saveAsTable`\n",
    "This writes a managed Delta table into the `default` database named `people_method1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31cd2125-d114-4576-9294-78b93bb863cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created default.people_method1\n+---+---------+----------+--------+------+-------------------+-----------+------+\n| id|firstName|middleName|lastName|gender|          birthDate|        ssn|salary|\n+---+---------+----------+--------+------+-------------------+-----------+------+\n|  2|     Mary|         A|   Smith|     F|1985-08-20 00:00:00|222-33-4444| 75000|\n|  1|     John|          |     Doe|     M|1990-05-14 00:00:00|111-22-3333| 60000|\n+---+---------+----------+--------+------+-------------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Method 1: saveAsTable (managed table)\n",
    "df.write.format('delta').mode('overwrite').saveAsTable('default.people_method1')\n",
    "print('Created default.people_method1')\n",
    "spark.sql(\"SELECT * FROM default.people_method1\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0ada679-481c-4934-b9bf-27fdf15fcc79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Delta Table — Method 2: `writeTo(...).createOrReplace()`\n",
    "This API is available in Spark 3.3+ with table support. It creates or replaces the table in the metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5fd8442-9824-4a5e-8af1-8286cc3ffac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created or replaced default.people_method2\n+---+---------+----------+--------+------+-------------------+-----------+------+\n| id|firstName|middleName|lastName|gender|          birthDate|        ssn|salary|\n+---+---------+----------+--------+------+-------------------+-----------+------+\n|  2|     Mary|         A|   Smith|     F|1985-08-20 00:00:00|222-33-4444| 75000|\n|  1|     John|          |     Doe|     M|1990-05-14 00:00:00|111-22-3333| 60000|\n+---+---------+----------+--------+------+-------------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Method 2: writeTo\n",
    "try:\n",
    "    df.writeTo('default.people_method2').createOrReplace()\n",
    "    print('Created or replaced default.people_method2')\n",
    "    spark.sql('SELECT * FROM default.people_method2').show()\n",
    "except Exception as e:\n",
    "    print('writeTo API may not be supported in this runtime:', e)\n",
    "    # fallback: saveAsTable\n",
    "    df.write.format('delta').mode('overwrite').saveAsTable('default.people_method2')\n",
    "    spark.sql('SELECT * FROM default.people_method2').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7669855e-3376-404c-a057-3333aa1846f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Delta Table — Method 3: Path-based Delta (external table)\n",
    "Write Delta files to a path and optionally register a table that points to that path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af41a763-c08c-4b18-aaac-1516dc518ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+-------------------+-----------+------+\n| id|firstName|middleName|lastName|gender|          birthDate|        ssn|salary|\n+---+---------+----------+--------+------+-------------------+-----------+------+\n|  2|     Mary|         A|   Smith|     F|1985-08-20 00:00:00|222-33-4444| 75000|\n|  1|     John|          |     Doe|     M|1990-05-14 00:00:00|111-22-3333| 60000|\n+---+---------+----------+--------+------+-------------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Method 3: path-based\n",
    "path = '/tmp/delta/people_method3'\n",
    "# write to path\n",
    "(df.write.format('delta')\n",
    "   .mode('overwrite')\n",
    "   .save(path))\n",
    "# Register as table (external)\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS default.people_method3 USING DELTA LOCATION '{path}'\")\n",
    "spark.sql('SELECT * FROM default.people_method3').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac48ca82-bf71-4406-9598-1592ce1afab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Merge & Upsert (SCD Type 1 example)\n",
    "We demonstrate MERGE to update existing rows and insert new rows (UPSERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5e2729-3a5d-4913-a4ba-2e7cbfc2b25e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+-------------------+-----------+------+\n| id|firstName|middleName|lastName|gender|          birthDate|        ssn|salary|\n+---+---------+----------+--------+------+-------------------+-----------+------+\n|  1|     John|          |     Doe|     M|1990-05-14 00:00:00|111-22-3333| 60000|\n|  2|     Mary|         A| Johnson|     F|1985-08-20 00:00:00|222-33-4444| 80000|\n|  3|    James|          |   Brown|     M|1992-01-10 00:00:00|333-44-5555| 65000|\n+---+---------+----------+--------+------+-------------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# prepare updates DataFrame\n",
    "updates = [\n",
    "    (2, 'Mary', 'A', 'Johnson', 'F', datetime(1985,8,20), '222-33-4444', 80000),  # update\n",
    "    (3, 'James', '', 'Brown', 'M', datetime(1992,1,10), '333-44-5555', 65000)        # insert\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(updates, schema)\n",
    "\n",
    "# Merge into people_method1\n",
    "deltaTable = DeltaTable.forName(spark, 'default.people_method1')\n",
    "(deltaTable.alias('t')\n",
    " .merge(updates_df.alias('u'), 't.id = u.id')\n",
    " .whenMatchedUpdateAll()\n",
    " .whenNotMatchedInsertAll()\n",
    " .execute())\n",
    "\n",
    "spark.sql('SELECT * FROM default.people_method1 ORDER BY id').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "263200ba-9801-4483-afda-84964f60c0db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Internals of Delta Table\n",
    "- Data files are stored as Parquet under the table location.\n",
    "- Transaction log is stored in `_delta_log/` as JSON and checkpoint Parquet files.\n",
    "- Delta keeps a commit history; you can inspect it with `deltaTable.history()` or `DESCRIBE HISTORY table`.\n",
    "\n",
    "Example commands below show how to inspect details and history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75916499-e8df-49ce-a413-6c5cae92cab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe detail:\n+------+------------------------------------+------------------------------------+-----------+----------------------------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------------------------------+---------------------------------------------------------------+-------------+\n|format|id                                  |name                                |description|location                                |createdAt              |lastModified       |partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties                           |minReaderVersion|minWriterVersion|tableFeatures                            |statistics                                                     |clusterByAuto|\n+------+------------------------------------+------------------------------------+-----------+----------------------------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------------------------------+---------------------------------------------------------------+-------------+\n|delta |fb71e2a4-4f5d-4747-b755-a7f0548a4b2b|spark_catalog.default.people_method1|NULL       |dbfs:/user/hive/warehouse/people_method1|2025-08-11 12:01:58.708|2025-08-11 12:04:34|[]              |[]               |3       |5977       |{delta.enableDeletionVectors -> true}|3               |7               |[appendOnly, deletionVectors, invariants]|{numRowsDeletedByDeletionVectors -> 0, numDeletionVectors -> 0}|false        |\n+------+------------------------------------+------------------------------------+-----------+----------------------------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------------------------------+---------------------------------------------------------------+-------------+\n\n+-------+-------------------+---------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId         |userName                          |operation                        |operationParameters                                                                                                                                                                                                 |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |userMetadata|engineInfo                                |\n+-------+-------------------+---------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|1      |2025-08-11 12:04:34|142195391229769|azuser4033_mml.local@techademy.com|MERGE                            |{predicate -> [\"(id#6873 = id#6849)\"], clusterBy -> [], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{3781192531290058}|0811-064430-yaxqaroa|0          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 3998, numTargetBytesRemoved -> 1993, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 2167, materializeSourceTimeMs -> 146, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1067, numTargetRowsUpdated -> 1, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 907}|NULL        |Databricks-Runtime/16.4.x-photon-scala2.12|\n|0      |2025-08-11 12:02:00|142195391229769|azuser4033_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}                                                            |NULL|{3781192531290058}|0811-064430-yaxqaroa|NULL       |WriteSerializable|false        |{numFiles -> 2, numRemovedFiles -> 0, numRemovedBytes -> 0, numOutputRows -> 2, numOutputBytes -> 3972}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |NULL        |Databricks-Runtime/16.4.x-photon-scala2.12|\n+-------+-------------------+---------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Inspect location and detail\n",
    "print('Describe detail:')\n",
    "spark.sql(\"DESCRIBE DETAIL default.people_method1\").show(truncate=False)\n",
    "\n",
    "# Use DeltaTable API to show history\n",
    "dt = DeltaTable.forName(spark, 'default.people_method1')\n",
    "try:\n",
    "    dt.history().show(truncate=False)\n",
    "except Exception as e:\n",
    "    print('history() may not be available in this runtime or requires permissions:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fe22e5c-df40-4ea1-bee1-046c9c178801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimize Delta Table\n",
    "`OPTIMIZE` and `ZORDER` are Databricks-specific commands (available on Databricks runtime) to compact files and colocate data for faster reads.\n",
    "\n",
    "Use `VACUUM` to remove old files after retention period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad6bf1b-b78b-429b-b2fa-11d6ba388412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZE executed (Databricks only)\nVACUUM executed (Databricks only)\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZE and VACUUM (Databricks runtime)\n",
    "try:\n",
    "    spark.sql('OPTIMIZE default.people_method1 ZORDER BY (id)')\n",
    "    print('OPTIMIZE executed (Databricks only)')\n",
    "except Exception as e:\n",
    "    print('OPTIMIZE may not be available in this runtime:', e)\n",
    "\n",
    "# VACUUM (requires that retention period be lower than default or set spark.conf)\n",
    "try:\n",
    "    spark.sql('VACUUM default.people_method1 RETAIN 168 HOURS')\n",
    "    print('VACUUM executed (Databricks only)')\n",
    "except Exception as e:\n",
    "    print('VACUUM may not be available or permission denied:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36fcb458-1f31-4b84-b1e6-3489d477ddad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Show tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d804ae84-0f98-4f0e-8332-7a89e2fd18aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----------+\n|database|     tableName|isTemporary|\n+--------+--------------+-----------+\n| default|        export|      false|\n| default|          loan|      false|\n| default|people_method1|      false|\n| default|people_method2|      false|\n| default|people_method3|      false|\n+--------+--------------+-----------+\n\n+------+------------------------------------+------------------------------------+-----------+----------------------------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------------------------------+---------------------------------------------------------------+-------------+\n|format|id                                  |name                                |description|location                                |createdAt              |lastModified       |partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties                           |minReaderVersion|minWriterVersion|tableFeatures                            |statistics                                                     |clusterByAuto|\n+------+------------------------------------+------------------------------------+-----------+----------------------------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------------------------------+---------------------------------------------------------------+-------------+\n|delta |fb71e2a4-4f5d-4747-b755-a7f0548a4b2b|spark_catalog.default.people_method1|NULL       |dbfs:/user/hive/warehouse/people_method1|2025-08-11 12:01:58.708|2025-08-11 12:06:20|[]              |[]               |1       |2165       |{delta.enableDeletionVectors -> true}|3               |7               |[appendOnly, deletionVectors, invariants]|{numRowsDeletedByDeletionVectors -> 0, numDeletionVectors -> 0}|false        |\n+------+------------------------------------+------------------------------------+-----------+----------------------------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------------------------------+---------------------------------------------------------------+-------------+\n\n+---+---------+----------+--------+------+-------------------+-----------+------+\n| id|firstName|middleName|lastName|gender|          birthDate|        ssn|salary|\n+---+---------+----------+--------+------+-------------------+-----------+------+\n|  2|     Mary|         A| Johnson|     F|1985-08-20 00:00:00|222-33-4444| 80000|\n|  3|    James|          |   Brown|     M|1992-01-10 00:00:00|333-44-5555| 65000|\n|  1|     John|          |     Doe|     M|1990-05-14 00:00:00|111-22-3333| 60000|\n+---+---------+----------+--------+------+-------------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW TABLES IN default').show()\n",
    "spark.sql(\"DESCRIBE DETAIL default.people_method1\").show(truncate=False)\n",
    "spark.sql(\"SELECT * FROM default.people_method1 LIMIT 20\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4e79f34-62f5-491a-84c5-044f32a9392c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DeltaLake-Assignment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}